{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7825024,"sourceType":"datasetVersion","datasetId":4585207}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-03-20T12:35:56.549174Z","iopub.execute_input":"2024-03-20T12:35:56.550363Z","iopub.status.idle":"2024-03-20T12:37:19.264461Z","shell.execute_reply.started":"2024-03-20T12:35:56.550317Z","shell.execute_reply":"2024-03-20T12:37:19.263178Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Exception ignored in: <function _xla_gc_callback at 0x79dd3ba90c10>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/jax/_src/lib/__init__.py\", line 97, in _xla_gc_callback\n    def _xla_gc_callback(*args):\nKeyboardInterrupt: \n\nKeyboardInterrupt\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torchviz\n!pip install torchview\n!pip install timm","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:34:45.121795Z","iopub.execute_input":"2024-03-20T12:34:45.122366Z","iopub.status.idle":"2024-03-20T12:35:30.538292Z","shell.execute_reply.started":"2024-03-20T12:34:45.122336Z","shell.execute_reply":"2024-03-20T12:35:30.537137Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchviz\n  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchviz) (2.1.2)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from torchviz) (0.20.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchviz) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchviz) (1.3.0)\nBuilding wheels for collected packages: torchviz\n  Building wheel for torchviz (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4131 sha256=1252ec788052803052bbc37374abacf510f9ee9559b80a90d437c7eb88da3e5b\n  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\nSuccessfully built torchviz\nInstalling collected packages: torchviz\nSuccessfully installed torchviz-0.0.2\nCollecting torchview\n  Downloading torchview-0.2.6-py3-none-any.whl.metadata (12 kB)\nDownloading torchview-0.2.6-py3-none-any.whl (25 kB)\nInstalling collected packages: torchview\nSuccessfully installed torchview-0.2.6\nRequirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (0.9.16)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from timm) (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.16.2)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm) (6.0.1)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.20.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (4.66.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (4.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->timm) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub->timm) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nimport h5py\nfrom torch import Tensor\nfrom typing import Type\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport torchvision\nfrom torchview import draw_graph\nfrom torchviz import make_dot\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom torchmetrics import Accuracy\nimport copy\nimport torch.optim as optim\nimport tqdm\nfrom tqdm import tqdm\nfrom torchmetrics import Accuracy\nfrom torch.utils.data import TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nfrom torchvision.datasets import ImageFolder\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:35:30.540364Z","iopub.execute_input":"2024-03-20T12:35:30.540713Z","iopub.status.idle":"2024-03-20T12:35:41.976825Z","shell.execute_reply.started":"2024-03-20T12:35:30.540679Z","shell.execute_reply":"2024-03-20T12:35:41.975954Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom PIL import Image\nimport multiprocessing\n\ndef load_images(folder_path):\n    images = []\n    for filename in os.listdir(folder_path):\n        img_path = os.path.join(folder_path, filename)\n        with Image.open(img_path) as img:\n            img = np.array(img)\n            images.append(img)\n    return images\n\ndef load_images_multiprocess(paths):\n    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n        images = pool.map(load_images, paths)\n    return images\n\npath_1 = '/kaggle/input/task2-png/DATA/Shashank/Task2Uncompressed/0/'\npath_2 = '/kaggle/input/task2-png/DATA/Shashank/Task2Uncompressed/1/'\n\nx_1, x_2 = load_images_multiprocess([path_1, path_2])\n\ny_1 = np.ones(len(x_1))\ny_2 = np.zeros(len(x_2))\n\nx = np.concatenate([x_1, x_2], axis=0)\ny = np.concatenate([y_1, y_2], axis=0)\n\ndel x_1, x_2, y_1, y_2\n","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:37:23.160975Z","iopub.execute_input":"2024-03-20T12:37:23.161755Z","iopub.status.idle":"2024-03-20T12:37:23.332680Z","shell.execute_reply.started":"2024-03-20T12:37:23.161723Z","shell.execute_reply":"2024-03-20T12:37:23.331208Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)","\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/opt/conda/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"/tmp/ipykernel_34/3396981761.py\", line 8, in load_images\n    for filename in os.listdir(folder_path):\nFileNotFoundError: [Errno 2] No such file or directory: '/kaggle/input/task2-png/DATA/Shashank/Task2Uncompressed/0/'\n\"\"\"","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m path_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/task2-png/DATA/Shashank/Task2Uncompressed/0/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     21\u001b[0m path_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/task2-png/DATA/Shashank/Task2Uncompressed/1/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 23\u001b[0m x_1, x_2 \u001b[38;5;241m=\u001b[39m \u001b[43mload_images_multiprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpath_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m y_1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(x_1))\n\u001b[1;32m     26\u001b[0m y_2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(x_2))\n","Cell \u001b[0;32mIn[5], line 17\u001b[0m, in \u001b[0;36mload_images_multiprocess\u001b[0;34m(paths)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_images_multiprocess\u001b[39m(paths):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m multiprocessing\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39mmultiprocessing\u001b[38;5;241m.\u001b[39mcpu_count()) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 17\u001b[0m         images \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m images\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/task2-png/DATA/Shashank/Task2Uncompressed/0/'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/task2-png/DATA/Shashank/Task2Uncompressed/0/'","output_type":"error"}]},{"cell_type":"code","source":"flattened_image = x.reshape((-1, 3))\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 3, 1)\nplt.hist(flattened_image[:, 0], bins=50, color='red', alpha=0.7)\nplt.title('Histogram of Channel 1')\nplt.xlabel('Pixel Value')\nplt.ylabel('Frequency')\n\nplt.subplot(1, 3, 2)\nplt.hist(flattened_image[:, 1], bins=50, color='blue', alpha=0.7)\nplt.title('Histogram of Channel 2')\nplt.xlabel('Pixel Value')\nplt.ylabel('Frequency')\n\nplt.subplot(1, 3, 3)\nplt.hist(flattened_image[:, 2], bins=50, color='blue', alpha=0.7)\nplt.title('Histogram of Channel 3')\nplt.xlabel('Pixel Value')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport math\nimport torch.nn as nn\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nclass BasicConv(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n        super(BasicConv, self).__init__()\n        self.out_channels = out_planes\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n        self.relu = nn.ReLU() if relu else None\n\n    def forward(self, x):\n        \n        x = self.conv(x)\n        \n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\nclass ChannelGate(nn.Module):\n    def __init__(self, gate_channels, reduction_ratio=16):\n        super(ChannelGate, self).__init__()\n        \n        # Number of input channels to image\n        self.gate_channels = gate_channels\n        \n        #MLP layer\n        self.mlp = nn.Sequential(\n            Flatten(),\n            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n            nn.ReLU(),\n            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n            )\n\n    def forward(self, x):\n        \n        #Avg_pool\n        avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n        channel_att_avg = self.mlp( avg_pool )\n\n        #max_pool\n        max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n        channel_att_max = self.mlp( max_pool )\n\n        #Element wise sum\n        channel_att_sum = channel_att_max + channel_att_avg\n\n        #scaling output of channel attention to match dimensions with input\n        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n        \n        return x * scale\n\n\nclass ChannelPool(nn.Module):\n    def forward(self, x):\n        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n\nclass SpatialGate(nn.Module):\n    def __init__(self):\n        super(SpatialGate, self).__init__()\n        kernel_size = 7\n        self.compress = ChannelPool()\n        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n        \n    def forward(self, x):\n        \n        # Applying Average Pooling and Maxpooling layer and concatenating\n        x_compress = self.compress(x)\n        \n        # Applying Convolution operation on concatenated inputs\n        x_out = self.spatial(x_compress)\n        \n        # Applying Sigmoid to attention mask\n        scale = F.sigmoid(x_out) # broadcasting\n        \n        return x * scale\n\nclass CBAM(nn.Module):\n    def __init__(self, gate_channels, reduction_ratio=1, no_spatial=False):\n        super(CBAM, self).__init__()\n        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio)\n        self.no_spatial=no_spatial\n        \n        if not no_spatial:\n            self.SpatialGate = SpatialGate()\n            \n    def forward(self, x):\n        x_out = self.ChannelGate(x)\n        if not self.no_spatial:\n            x_out = self.SpatialGate(x_out)\n        return x_out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n\n# class BasicBlock(nn.Module):\n#     def __init__(self, in_channels, out_channels):\n#         super(BasicBlock, self).__init__()\n#         self.conv_1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding='same')\n#         self.bn_1 = nn.BatchNorm2d(out_channels)\n#         self.conv_2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding='same')\n#         self.bn_2 = nn.BatchNorm2d(out_channels)\n#         self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n#         self.relu = nn.ReLU()\n        \n#     def forward(self, x):\n#         x = self.conv_1(x)\n#         x = self.bn_1(x)\n#         x = self.relu(x)\n#         x = self.conv_2(x)\n#         x = self.bn_2(x)\n#         x = self.relu(x)\n#         x = self.maxpool(x)\n#         return x\n\n# class VGG_12(nn.Module):\n#     def __init__(self, BasicBlock, in_channels, out_channels, CBAM):\n#         super(VGG_12, self).__init__()\n#         self.in_channels = in_channels\n#         self.out_channels = out_channels\n#         self.block_1 = self.__make_layer(BasicBlock, change = 'yes')\n#         self.cbam_1 = self.channel_block_attention(CBAM)\n#         self.block_2 = self.__make_layer(BasicBlock, change = 'yes')\n#         self.cbam_2 = self.channel_block_attention(CBAM)\n#         self.block_3 = self.__make_layer(BasicBlock, change = 'yes')\n#         self.cbam_3 = self.channel_block_attention(CBAM)\n#         self.block_4 = self.__make_layer(BasicBlock, change = 'NO')\n#         self.cbam_4 = self.channel_block_attention(CBAM)\n#         self.block_5 = self.__make_layer(BasicBlock, change = 'NO')\n#         self.fc_1 = nn.Linear(in_features=128, out_features=64)  # Adjust in_features according to the output shape after blocks\n# #         self.fc_2 = nn.Linear(in_features=512, out_features=64)\n#         self.fc_3 = nn.Linear(in_features=64, out_features=1)\n#         self.relu = nn.ReLU()\n#         self.sigmoid = nn.Sigmoid()\n#         self.dropout = nn.Dropout(p = 0.2)\n#         self.avg_pool = nn.AvgPool2d(kernel_size = 3)\n        \n#     def __make_layer(self, block, change):\n#         op =  block(in_channels=self.in_channels, out_channels=self.out_channels)\n#         self.in_channels = self.out_channels\n#         if change == 'yes':\n#             self.out_channels = self.out_channels*2\n#         return op\n    \n#     def channel_block_attention(self, cbam):\n#         return cbam(self.in_channels)\n    \n#     def forward(self, x):\n#         x = self.block_1(x)\n#         x = self.cbam_1(x)\n# #         x = self.relu(x)\n        \n#         x = self.block_2(x)\n#         x = self.cbam_2(x)\n# #         x = self.relu(x)\n        \n#         x = self.block_3(x)\n#         x = self.cbam_3(x)\n# #         x = self.relu(x)\n        \n#         x = self.block_4(x)\n#         x = self.cbam_4(x)\n# #         x = self.relu(x)\n        \n#         x = self.block_5(x)\n# #         x = self.relu(x)\n\n#         x = self.avg_pool(x)\n#         x = x.view(x.size(0), -1)\n#         x = self.fc_1(x)\n# #         x = self.dropout(x)\n#         x = self.relu(x)\n        \n#         x = self.fc_3(x)\n#         x = self.sigmoid(x)\n#         return x\n\nclass VGG16(nn.Module):\n    def __init__(self, num_classes=10):\n        super(VGG16, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU())\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(), \n            nn.MaxPool2d(kernel_size = 2, stride = 2))\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU())\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2, stride = 2))\n        self.layer5 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU())\n        self.layer6 = nn.Sequential(\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU())\n        self.layer7 = nn.Sequential(\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2, stride = 2))\n        self.layer8 = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU())\n        self.layer9 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU())\n        self.layer10 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2, stride = 2))\n        self.layer11 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU())\n        self.layer12 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU())\n        self.layer13 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2, stride = 2))\n        self.fc = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(7*7*512, 4096),\n            nn.ReLU())\n        self.fc1 = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU())\n        self.fc2= nn.Sequential(\n            nn.Linear(4096, num_classes))\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n        out = self.layer6(out)\n        out = self.layer7(out)\n        out = self.layer8(out)\n        out = self.layer9(out)\n        out = self.layer10(out)\n#         out = self.layer11(out)\n#         out = self.layer12(out)\n#         out = self.layer13(out)\n        out = out.reshape(out.size(0), -1)\n#         out = self.fc(out)\n#         out = self.fc1(out)\n#         out = self.fc2(out)\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = VGG_12(BasicBlock, 3, 16, CBAM)\nmodel_graph = draw_graph(model, input_size=(1,3,125,125), expand_nested=True)\nmodel_graph.visual_graph","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, x, y, transform = None):\n        self.x = x\n        self.y = y\n        self.transform = transform\n        \n    def __len__(self):\n        return self.x.shape[0]\n    \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        img = self.x[idx]\n        label = self.y[idx]\n        label = torch.tensor(label).float().unsqueeze(0)\n        if self.transform:\n            img = self.transform(img)\n        \n        sample = {'image':img, 'labels' : label}\n        \n        return sample\n            \n# Define transformations to apply to the images\ntransform = transforms.Compose([\n    # torchvision.transforms.ToPILImage(),\n    # transforms.RandomHorizontalFlip(),   # Randomly flip the image horizontally\n    # transforms.RandomVerticalFlip(),     # Randomly flip the image vertically\n    # transforms.RandomRotation(10),       # Randomly rotate the image by up to 10 degrees\n    transforms.ToTensor()                # Convert PIL Image to tensor\n])\n\n\n# Create the ImageFolder dataset\ndataset = CustomDataset(x, y, transform = transform)\n\nsample = dataset.__getitem__(0)\nprint(sample['image'].shape)\nprint(sample['labels'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model\nimport gc\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\ngc.collect()\nlibc.malloc_trim(0)\ntorch.cuda.empty_cache()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_train(fold, model, epochs, train_dataloader, test_dataloader):\n    \n    # --------------------Loss function and optimizer--------------------\n    criterion = nn.BCELoss()  # Binary Cross Entropy\n    optimizer = optim.Adam(model.parameters(), lr=0.01)\n    # -------------------------------------------------------------------\n\n    best_acc = -np.inf  # Init to negative infinity\n    best_weights = None\n    accuracy = Accuracy(task = 'binary').to(DEVICE)\n \n    train_losses = []\n    val_losses = []\n    train_accuracies = []\n    val_accuracies = []\n\n    for epoch in range(epochs):\n        train_pred = []\n        val_pred = []\n        \n        # --------------------Training Loop--------------------\n        model.train()\n        for batch in tqdm(train_dataloader):\n            images, labels = batch['image'], batch['labels']\n            images = images.to(DEVICE)\n            labels = labels.to(DEVICE)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_pred.append(loss.item())\n\n            # Calculate training accuracy\n            train_acc = accuracy(outputs, labels)\n            train_accuracies.append(train_acc.item())\n\n        train_loss = np.mean(train_pred)\n        # -------------------------------------------------------\n        # -------------------------------------------------------\n        \n        # --------------------Validation Loop--------------------\n        model.eval()\n        with torch.no_grad():\n            for val_batch in tqdm(test_dataloader):\n                val_images, val_labels = val_batch['image'], val_batch['labels']\n                val_images = val_images.to(DEVICE)\n                val_labels = val_labels.to(DEVICE)\n\n                val_outputs = model(val_images)\n                val_loss = criterion(val_outputs, val_labels)\n                val_pred.append(val_loss.item())\n\n                # Calculate validation accuracy\n                val_acc = accuracy(val_outputs, val_labels)\n                val_accuracies.append(val_acc.item())\n\n        val_loss = np.mean(val_pred)\n        # -------------------------------------------------------\n        # -------------------------------------------------------\n        \n        # Print and store losses and accuracies\n        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {np.mean(train_accuracies):.4f}, Valid Loss: {val_loss:.4f}, Valid Accuracy: {np.mean(val_accuracies):.4f}')\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n\n        with open('vgg_loss.txt', 'a') as f:  # Open file in append mode\n            f.write(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\\n')\n            \n        # Save best model\n        if max(train_accuracies) > best_acc:\n            best_acc = max(train_accuracies)\n            best_weights = copy.deepcopy(model.state_dict())\n    \n    # Save the best model\n    torch.save(best_weights, f'./best_model_{fold}.pth')\n\n    # Plot training and validation losses\n    return train_losses, val_losses, train_accuracies, val_accuracies\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del model\nimport gc\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\ngc.collect()\nlibc.malloc_trim(0)\ntorch.cuda.empty_cache()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_folds = 5\nbatch_size = 256\n\n# -------------------------------------------------------\n#                   Training KFold Starts\n# -------------------------------------------------------\nkf = KFold(n_splits=k_folds, shuffle=True)\n\ntrn_fold_loss = []\nval_fold_loss = []\n\nfor fold, (train_idx, test_idx) in enumerate(kf.split(dataset)):\n    \n    print(f'*** Fold {fold}***')\n    print('Training Started.....')\n    \n    train_loader = DataLoader(dataset=dataset,batch_size=batch_size,sampler=torch.utils.data.SubsetRandomSampler(train_idx))\n    test_loader = DataLoader(dataset=dataset,batch_size=batch_size,sampler=torch.utils.data.SubsetRandomSampler(test_idx))\n    \n    DEVICE = torch.device(\"cuda\")\n    model = VGG_12(BasicBlock, 3, 16, CBAM)\n    \n    NUM_GPU = torch.cuda.device_count()\n    if NUM_GPU > 1:\n        model = nn.DataParallel(model)\n    model = model.to(DEVICE)\n    \n    trn_loss, val_loss, trn_acc, val_acc = model_train(fold, model, epochs = 10, train_dataloader = train_loader, test_dataloader = test_loader)\n    trn_fold_loss.append(trn_loss)\n    val_fold_loss.append(val_loss)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}