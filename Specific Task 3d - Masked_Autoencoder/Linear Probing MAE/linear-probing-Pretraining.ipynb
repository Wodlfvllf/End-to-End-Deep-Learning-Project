{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b900e881",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-18T12:20:23.345871Z",
     "iopub.status.busy": "2024-03-18T12:20:23.345238Z",
     "iopub.status.idle": "2024-03-18T12:20:23.350069Z",
     "shell.execute_reply": "2024-03-18T12:20:23.349378Z"
    },
    "papermill": {
     "duration": 0.016212,
     "end_time": "2024-03-18T12:20:23.351947",
     "exception": false,
     "start_time": "2024-03-18T12:20:23.335735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f097c0b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T12:20:23.368916Z",
     "iopub.status.busy": "2024-03-18T12:20:23.368276Z",
     "iopub.status.idle": "2024-03-18T12:20:54.036399Z",
     "shell.execute_reply": "2024-03-18T12:20:54.035358Z"
    },
    "papermill": {
     "duration": 30.67913,
     "end_time": "2024-03-18T12:20:54.038903",
     "exception": false,
     "start_time": "2024-03-18T12:20:23.359773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchview\r\n",
      "  Downloading torchview-0.2.6-py3-none-any.whl.metadata (12 kB)\r\n",
      "Downloading torchview-0.2.6-py3-none-any.whl (25 kB)\r\n",
      "Installing collected packages: torchview\r\n",
      "Successfully installed torchview-0.2.6\r\n",
      "Collecting torchviz\r\n",
      "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchviz) (2.1.2)\r\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from torchviz) (0.20.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (2024.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchviz) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchviz) (1.3.0)\r\n",
      "Building wheels for collected packages: torchviz\r\n",
      "  Building wheel for torchviz (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4131 sha256=3d39127ff6a7e7c498734d2f60c4f6d8e5999db1ebd3ef5f5ccfd98b2ef843f5\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\r\n",
      "Successfully built torchviz\r\n",
      "Installing collected packages: torchviz\r\n",
      "Successfully installed torchviz-0.0.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchview\n",
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "309858a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T12:20:54.058948Z",
     "iopub.status.busy": "2024-03-18T12:20:54.058630Z",
     "iopub.status.idle": "2024-03-18T12:21:02.055281Z",
     "shell.execute_reply": "2024-03-18T12:21:02.054318Z"
    },
    "papermill": {
     "duration": 8.00921,
     "end_time": "2024-03-18T12:21:02.057422",
     "exception": false,
     "start_time": "2024-03-18T12:20:54.048212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7b066d371550>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import ctypes\n",
    "import gc\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "016b6db9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T12:21:02.077614Z",
     "iopub.status.busy": "2024-03-18T12:21:02.077238Z",
     "iopub.status.idle": "2024-03-18T12:22:12.879824Z",
     "shell.execute_reply": "2024-03-18T12:22:12.878968Z"
    },
    "papermill": {
     "duration": 70.815391,
     "end_time": "2024-03-18T12:22:12.882222",
     "exception": false,
     "start_time": "2024-03-18T12:21:02.066831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups in the HDF5 file:\n",
      "Y\n",
      "jet\n",
      "Dataset shape: (10000, 125, 125, 8)\n",
      "Dataset dtype: float32\n",
      "Dataset shape: (10000, 1)\n",
      "Dataset dtype: float32\n",
      "Dataset attributes:\n"
     ]
    }
   ],
   "source": [
    "# Open the HDF5 file\n",
    "import h5py\n",
    "with h5py.File('/kaggle/input/autoencoders-labelled/Dataset_Specific_labelled.h5', 'r') as file:\n",
    "    # List all the groups in the file\n",
    "    print(\"Groups in the HDF5 file:\")\n",
    "    for group in file:\n",
    "        print(group)\n",
    "\n",
    "    # Get information about a specific dataset\n",
    "    dataset = file['jet']\n",
    "    print(\"Dataset shape:\", dataset.shape)\n",
    "    print(\"Dataset dtype:\", dataset.dtype)\n",
    "    \n",
    "    dataset = file['Y']\n",
    "    print(\"Dataset shape:\", dataset.shape)\n",
    "    print(\"Dataset dtype:\", dataset.dtype)\n",
    "\n",
    "\n",
    "    # Explore attributes of the dataset\n",
    "    print(\"Dataset attributes:\")\n",
    "    for attr_name, attr_value in dataset.attrs.items():\n",
    "        print(f\"{attr_name}: {attr_value}\")\n",
    "\n",
    "    X = np.array(file['jet'][:])\n",
    "    Y = np.array(file['Y'][:])\n",
    "    # Explore more datasets, groups, and attributes as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "217880ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T12:22:12.902263Z",
     "iopub.status.busy": "2024-03-18T12:22:12.901942Z",
     "iopub.status.idle": "2024-03-18T12:22:12.907467Z",
     "shell.execute_reply": "2024-03-18T12:22:12.906652Z"
    },
    "papermill": {
     "duration": 0.017601,
     "end_time": "2024-03-18T12:22:12.909487",
     "exception": false,
     "start_time": "2024-03-18T12:22:12.891886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 125, 125, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d592343",
   "metadata": {
    "papermill": {
     "duration": 0.008964,
     "end_time": "2024-03-18T12:22:12.928943",
     "exception": false,
     "start_time": "2024-03-18T12:22:12.919979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42a75301",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T12:22:12.948572Z",
     "iopub.status.busy": "2024-03-18T12:22:12.948317Z",
     "iopub.status.idle": "2024-03-18T12:22:12.959282Z",
     "shell.execute_reply": "2024-03-18T12:22:12.958638Z"
    },
    "papermill": {
     "duration": 0.022831,
     "end_time": "2024-03-18T12:22:12.961123",
     "exception": false,
     "start_time": "2024-03-18T12:22:12.938292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype='float32')\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f64a219",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T12:22:12.980972Z",
     "iopub.status.busy": "2024-03-18T12:22:12.980714Z",
     "iopub.status.idle": "2024-03-18T12:22:25.667341Z",
     "shell.execute_reply": "2024-03-18T12:22:25.666504Z"
    },
    "papermill": {
     "duration": 12.699458,
     "end_time": "2024-03-18T12:22:25.669760",
     "exception": false,
     "start_time": "2024-03-18T12:22:12.970302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=8,\n",
    "                 embed_dim=1024, depth=24, num_heads=16,\n",
    "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE encoder specifics\n",
    "        self.mask_ratio = 0.75\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        \n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        \n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 8, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 8, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 8))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 8, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 8))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 8, h * p, h * p))\n",
    "        return imgs\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        imgs = self.patchify(x)\n",
    "        \n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, self.mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return x, mask, ids_restore, imgs\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=8,\n",
    "                 embed_dim=1024, depth=24, num_heads=16,\n",
    "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_patches = (img_size//patch_size)**2\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "\n",
    "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.num_patches**.5), cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 8, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 8, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 8))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 8, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 8))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 8, h * p, h * p))\n",
    "        return imgs\n",
    "\n",
    "    def forward(self, x, ids_restore):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Masked_VIT(nn.Module):\n",
    "    def __init__(self, encoder, decoder, mask_ratio):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.mask_ratio = mask_ratio\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, mask, ids_restore, imgs = self.encoder(x)\n",
    "        pred = self.decoder(x, ids_restore)\n",
    "        \n",
    "        return imgs, pred, mask\n",
    "    \n",
    "def mae_vit_base_patch16_dec512d8b(img_size=125, mask_ratio = 0.75, **kwargs):\n",
    "    encoder = Encoder(\n",
    "        img_size=img_size, patch_size=5, embed_dim=768, depth=8, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=4, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    \n",
    "    decoder = Decoder(\n",
    "        img_size=img_size, patch_size=5, embed_dim=768, depth=8, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=4, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    \n",
    "    model = Masked_VIT(encoder, decoder, mask_ratio)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "model = mae_vit_base_patch16_dec512d8b(img_size=125, mask_ratio = 0.75)\n",
    "model = torch.load('/kaggle/input/pretrained-weights-autoencoder/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3b03093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T12:22:25.690529Z",
     "iopub.status.busy": "2024-03-18T12:22:25.689633Z",
     "iopub.status.idle": "2024-03-18T12:22:38.307798Z",
     "shell.execute_reply": "2024-03-18T12:22:38.306759Z"
    },
    "papermill": {
     "duration": 12.630635,
     "end_time": "2024-03-18T12:22:38.310198",
     "exception": false,
     "start_time": "2024-03-18T12:22:25.679563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\r\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m737.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: einops\r\n",
      "Successfully installed einops-0.7.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3813db4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T12:22:38.331689Z",
     "iopub.status.busy": "2024-03-18T12:22:38.331377Z",
     "iopub.status.idle": "2024-03-18T12:22:38.335496Z",
     "shell.execute_reply": "2024-03-18T12:22:38.334750Z"
    },
    "papermill": {
     "duration": 0.016774,
     "end_time": "2024-03-18T12:22:38.337231",
     "exception": false,
     "start_time": "2024-03-18T12:22:38.320457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.module.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84f8e242",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T12:22:38.357843Z",
     "iopub.status.busy": "2024-03-18T12:22:38.357578Z",
     "iopub.status.idle": "2024-03-18T12:22:38.379065Z",
     "shell.execute_reply": "2024-03-18T12:22:38.378202Z"
    },
    "papermill": {
     "duration": 0.033973,
     "end_time": "2024-03-18T12:22:38.381079",
     "exception": false,
     "start_time": "2024-03-18T12:22:38.347106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import repeat, rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "class VIT_classifier(nn.Module):\n",
    "    def __init__(self, encoder, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.patch_embed = encoder.patch_embed\n",
    "        self.cls_token = encoder.cls_token\n",
    "        self.pos_embed = encoder.pos_embed  # Corrected attribute name\n",
    "        self.patchify = encoder.patchify\n",
    "        self.transformer = encoder.blocks  # Corrected attribute name\n",
    "        self.layer_norm = encoder.norm  # Corrected attribute name\n",
    "        self.head = torch.nn.Linear(self.pos_embed.shape[-1], num_classes)\n",
    "        self.blocks = encoder.blocks\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d((1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(in_features=625, out_features=64)\n",
    "        self.fc_1 = nn.Linear(in_features=64, out_features=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        \n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        x = x[:,1:,:]\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.fc_1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "encoder = model.encoder\n",
    "classifier = VIT_classifier(encoder, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5c9dc45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T12:22:38.401700Z",
     "iopub.status.busy": "2024-03-18T12:22:38.401428Z",
     "iopub.status.idle": "2024-03-18T12:22:38.405422Z",
     "shell.execute_reply": "2024-03-18T12:22:38.404612Z"
    },
    "papermill": {
     "duration": 0.016424,
     "end_time": "2024-03-18T12:22:38.407251",
     "exception": false,
     "start_time": "2024-03-18T12:22:38.390827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torchvision\n",
    "# from torchview import draw_graph\n",
    "# from torchviz import make_dot\n",
    "\n",
    "# # # encoder = Encoder(\n",
    "# # #         img_size=125, patch_size=5, embed_dim=768, depth=8, num_heads=12,\n",
    "# # #         decoder_embed_dim=512, decoder_depth=4, decoder_num_heads=16,\n",
    "# # #         mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6)\n",
    "# # # encoder = model.encoder\n",
    "\n",
    "# model_graph = draw_graph(classifier, input_size=(1,8,125,125), expand_nested=True)\n",
    "# model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5bbe02c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T12:22:38.427833Z",
     "iopub.status.busy": "2024-03-18T12:22:38.427595Z",
     "iopub.status.idle": "2024-03-18T12:22:38.431251Z",
     "shell.execute_reply": "2024-03-18T12:22:38.430389Z"
    },
    "papermill": {
     "duration": 0.01627,
     "end_time": "2024-03-18T12:22:38.433241",
     "exception": false,
     "start_time": "2024-03-18T12:22:38.416971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7523d921",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T12:22:38.454403Z",
     "iopub.status.busy": "2024-03-18T12:22:38.454161Z",
     "iopub.status.idle": "2024-03-18T12:22:58.914114Z",
     "shell.execute_reply": "2024-03-18T12:22:58.912840Z"
    },
    "papermill": {
     "duration": 20.473612,
     "end_time": "2024-03-18T12:22:58.916577",
     "exception": false,
     "start_time": "2024-03-18T12:22:38.442965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for _z in range(8):\n",
    "    X[:,:,:,_z] = (X[:,:,:,_z] - X[:,:,:,_z].mean()) / (X[:,:,:,_z].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2dd1830",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T12:22:58.938413Z",
     "iopub.status.busy": "2024-03-18T12:22:58.937770Z",
     "iopub.status.idle": "2024-03-18T12:23:00.090231Z",
     "shell.execute_reply": "2024-03-18T12:23:00.088994Z"
    },
    "papermill": {
     "duration": 1.165824,
     "end_time": "2024-03-18T12:23:00.092584",
     "exception": false,
     "start_time": "2024-03-18T12:22:58.926760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 125, 125])\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, x, y, transform):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_1 = self.x[idx]\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img_1 = self.transform(img_1)\n",
    "            \n",
    "        sample = {'img' : img_1, 'label' : label}\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "dataset = Custom_Dataset(X, Y, transform = transform)\n",
    "sample = dataset.__getitem__(0)\n",
    "print((sample['img']).shape)\n",
    "print(sample['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d133e0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T12:23:00.115001Z",
     "iopub.status.busy": "2024-03-18T12:23:00.113786Z",
     "iopub.status.idle": "2024-03-18T12:23:00.118437Z",
     "shell.execute_reply": "2024-03-18T12:23:00.117527Z"
    },
    "papermill": {
     "duration": 0.017304,
     "end_time": "2024-03-18T12:23:00.120274",
     "exception": false,
     "start_time": "2024-03-18T12:23:00.102970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9895772f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T12:23:00.141495Z",
     "iopub.status.busy": "2024-03-18T12:23:00.141223Z",
     "iopub.status.idle": "2024-03-18T12:23:04.991945Z",
     "shell.execute_reply": "2024-03-18T12:23:04.991184Z"
    },
    "papermill": {
     "duration": 4.864107,
     "end_time": "2024-03-18T12:23:04.994339",
     "exception": false,
     "start_time": "2024-03-18T12:23:00.130232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from torch import Tensor\n",
    "from typing import Type\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torchvision\n",
    "from torchview import draw_graph\n",
    "from torchviz import make_dot\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torchmetrics import Accuracy\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.data import TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def model_train(fold, model, epochs, train_dataloader, test_dataloader):\n",
    "    \n",
    "    # --------------------Loss function and optimizer--------------------\n",
    "    criterion = nn.BCELoss()  # Binary Cross Entropy\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1.5e-5)\n",
    "    # -------------------------------------------------------------------\n",
    "\n",
    "    best_acc = -np.inf  # Init to negative infinity\n",
    "    best_weights = None\n",
    "    accuracy = Accuracy(task = 'binary').to(DEVICE)\n",
    " \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_pred = []\n",
    "        val_pred = []\n",
    "        \n",
    "        # --------------------Training Loop--------------------\n",
    "        model.train()\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            images, labels = batch['img'], batch['label']\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_pred.append(loss.item())\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            train_acc = accuracy(outputs, labels)\n",
    "            train_accuracies.append(train_acc.item())\n",
    "\n",
    "        train_loss = np.mean(train_pred)\n",
    "        # -------------------------------------------------------\n",
    "        # -------------------------------------------------------\n",
    "        \n",
    "        # --------------------Validation Loop--------------------\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_batch in tqdm(test_dataloader):\n",
    "                val_images, val_labels = val_batch['img'], val_batch['label']\n",
    "                val_images = val_images.to(DEVICE)\n",
    "                val_labels = val_labels.to(DEVICE)\n",
    "\n",
    "                val_outputs = model(val_images)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "                val_pred.append(val_loss.item())\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                val_acc = accuracy(val_outputs, val_labels)\n",
    "                val_accuracies.append(val_acc.item())\n",
    "\n",
    "        val_loss = np.mean(val_pred)\n",
    "        # -------------------------------------------------------\n",
    "        # -------------------------------------------------------\n",
    "        \n",
    "        # Print and store losses and accuracies\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {np.mean(train_accuracies):.4f}, Valid Loss: {val_loss:.4f}, Valid Accuracy: {np.mean(val_accuracies):.4f}')\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Save best model\n",
    "        if max(train_accuracies) > best_acc:\n",
    "            best_acc = max(train_accuracies)\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # Save the best model\n",
    "    torch.save(best_weights, f'./best_model_{fold}.pth')\n",
    "\n",
    "    # Plot training and validation losses\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a412e61a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T12:23:05.016444Z",
     "iopub.status.busy": "2024-03-18T12:23:05.015577Z",
     "iopub.status.idle": "2024-03-18T12:23:05.211295Z",
     "shell.execute_reply": "2024-03-18T12:23:05.210185Z"
    },
    "papermill": {
     "duration": 0.208761,
     "end_time": "2024-03-18T12:23:05.213347",
     "exception": false,
     "start_time": "2024-03-18T12:23:05.004586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del classifier\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6c60237",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T12:23:05.234887Z",
     "iopub.status.busy": "2024-03-18T12:23:05.234579Z",
     "iopub.status.idle": "2024-03-18T12:23:05.238416Z",
     "shell.execute_reply": "2024-03-18T12:23:05.237635Z"
    },
    "papermill": {
     "duration": 0.016447,
     "end_time": "2024-03-18T12:23:05.240186",
     "exception": false,
     "start_time": "2024-03-18T12:23:05.223739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = mae_vit_base_patch16_dec512d8b(img_size=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03b09372",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T12:23:05.261048Z",
     "iopub.status.busy": "2024-03-18T12:23:05.260774Z",
     "iopub.status.idle": "2024-03-18T13:10:52.535325Z",
     "shell.execute_reply": "2024-03-18T13:10:52.534458Z"
    },
    "papermill": {
     "duration": 2867.287474,
     "end_time": "2024-03-18T13:10:52.537385",
     "exception": false,
     "start_time": "2024-03-18T12:23:05.249911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [05:24<00:00,  2.59s/it]\n",
      "100%|██████████| 32/32 [00:26<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8, Train Loss: 0.6876, Train Accuracy: 0.5116, Valid Loss: 0.6572, Valid Accuracy: 0.8130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [05:33<00:00,  2.67s/it]\n",
      "100%|██████████| 32/32 [00:26<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/8, Train Loss: 0.6066, Train Accuracy: 0.6817, Valid Loss: 0.5604, Valid Accuracy: 0.8342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [05:32<00:00,  2.66s/it]\n",
      "100%|██████████| 32/32 [00:26<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/8, Train Loss: 0.5186, Train Accuracy: 0.7469, Valid Loss: 0.4897, Valid Accuracy: 0.8447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [05:32<00:00,  2.66s/it]\n",
      "100%|██████████| 32/32 [00:26<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/8, Train Loss: 0.4522, Train Accuracy: 0.7807, Valid Loss: 0.4434, Valid Accuracy: 0.8486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [05:32<00:00,  2.66s/it]\n",
      "100%|██████████| 32/32 [00:26<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/8, Train Loss: 0.3829, Train Accuracy: 0.8050, Valid Loss: 0.4104, Valid Accuracy: 0.8499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [05:32<00:00,  2.66s/it]\n",
      "100%|██████████| 32/32 [00:26<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/8, Train Loss: 0.3303, Train Accuracy: 0.8234, Valid Loss: 0.3832, Valid Accuracy: 0.8523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [05:32<00:00,  2.66s/it]\n",
      "100%|██████████| 32/32 [00:26<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/8, Train Loss: 0.2872, Train Accuracy: 0.8378, Valid Loss: 0.3744, Valid Accuracy: 0.8541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [05:32<00:00,  2.66s/it]\n",
      "100%|██████████| 32/32 [00:26<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/8, Train Loss: 0.2604, Train Accuracy: 0.8494, Valid Loss: 0.3750, Valid Accuracy: 0.8548\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "classifier = VIT_classifier(model.encoder, 2)\n",
    "NUM_GPU = torch.cuda.device_count()\n",
    "if NUM_GPU > 1:\n",
    "    classifier = nn.DataParallel(classifier)\n",
    "classifier = classifier.to(DEVICE)\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = model_train(1,classifier, 8, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7f39875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:52.766130Z",
     "iopub.status.busy": "2024-03-18T13:10:52.765500Z",
     "iopub.status.idle": "2024-03-18T13:10:52.769719Z",
     "shell.execute_reply": "2024-03-18T13:10:52.768821Z"
    },
    "papermill": {
     "duration": 0.122605,
     "end_time": "2024-03-18T13:10:52.771750",
     "exception": false,
     "start_time": "2024-03-18T13:10:52.649145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.save(model, f'./best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "546bd85f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:52.999009Z",
     "iopub.status.busy": "2024-03-18T13:10:52.998723Z",
     "iopub.status.idle": "2024-03-18T13:10:53.002795Z",
     "shell.execute_reply": "2024-03-18T13:10:53.001931Z"
    },
    "papermill": {
     "duration": 0.117202,
     "end_time": "2024-03-18T13:10:53.004601",
     "exception": false,
     "start_time": "2024-03-18T13:10:52.887399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder = classifier.module.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d47bd91f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:53.227906Z",
     "iopub.status.busy": "2024-03-18T13:10:53.227504Z",
     "iopub.status.idle": "2024-03-18T13:10:53.537327Z",
     "shell.execute_reply": "2024-03-18T13:10:53.536295Z"
    },
    "papermill": {
     "duration": 0.425712,
     "end_time": "2024-03-18T13:10:53.540105",
     "exception": false,
     "start_time": "2024-03-18T13:10:53.114393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(encoder, 'encoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "385b6b02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:53.778555Z",
     "iopub.status.busy": "2024-03-18T13:10:53.778181Z",
     "iopub.status.idle": "2024-03-18T13:10:54.080897Z",
     "shell.execute_reply": "2024-03-18T13:10:54.079627Z"
    },
    "papermill": {
     "duration": 0.426474,
     "end_time": "2024-03-18T13:10:54.083325",
     "exception": false,
     "start_time": "2024-03-18T13:10:53.656851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(classifier.module, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b52c768d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:54.312943Z",
     "iopub.status.busy": "2024-03-18T13:10:54.312339Z",
     "iopub.status.idle": "2024-03-18T13:10:54.651372Z",
     "shell.execute_reply": "2024-03-18T13:10:54.650423Z"
    },
    "papermill": {
     "duration": 0.456672,
     "end_time": "2024-03-18T13:10:54.653330",
     "exception": false,
     "start_time": "2024-03-18T13:10:54.196658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0237897f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:54.878077Z",
     "iopub.status.busy": "2024-03-18T13:10:54.877694Z",
     "iopub.status.idle": "2024-03-18T13:10:54.891249Z",
     "shell.execute_reply": "2024-03-18T13:10:54.890377Z"
    },
    "papermill": {
     "duration": 0.127796,
     "end_time": "2024-03-18T13:10:54.893047",
     "exception": false,
     "start_time": "2024-03-18T13:10:54.765251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class MaskedAutoencoderViT(nn.Module):\n",
    "#     \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
    "#     \"\"\"\n",
    "#     def __init__(self, img_size=224, patch_size=16, in_chans=8,\n",
    "#                  embed_dim=1024, depth=24, num_heads=16,\n",
    "#                  decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "#                  mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # --------------------------------------------------------------------------\n",
    "#         # MAE encoder specifics\n",
    "#         self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "#         num_patches = self.patch_embed.num_patches\n",
    "\n",
    "#         self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "#         self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "#         self.blocks = nn.ModuleList([\n",
    "#             Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "#             for i in range(depth)])\n",
    "#         self.norm = norm_layer(embed_dim)\n",
    "#         # --------------------------------------------------------------------------\n",
    "\n",
    "#         # --------------------------------------------------------------------------\n",
    "#         # MAE decoder specifics\n",
    "#         self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "#         self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "#         self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "#         self.decoder_blocks = nn.ModuleList([\n",
    "#             Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "#             for i in range(decoder_depth)])\n",
    "\n",
    "#         self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "#         self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
    "#         # --------------------------------------------------------------------------\n",
    "\n",
    "#         self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "#         self.initialize_weights()\n",
    "\n",
    "#     def initialize_weights(self):\n",
    "#         # initialization\n",
    "#         # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "#         pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "#         self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "#         decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "#         self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "#         # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "#         w = self.patch_embed.proj.weight.data\n",
    "#         torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "#         # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "#         torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "#         torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "#         # initialize nn.Linear and nn.LayerNorm\n",
    "#         self.apply(self._init_weights)\n",
    "\n",
    "#     def _init_weights(self, m):\n",
    "#         if isinstance(m, nn.Linear):\n",
    "#             # we use xavier_uniform following official JAX ViT:\n",
    "#             torch.nn.init.xavier_uniform_(m.weight)\n",
    "#             if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "#         elif isinstance(m, nn.LayerNorm):\n",
    "#             nn.init.constant_(m.bias, 0)\n",
    "#             nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "#     def patchify(self, imgs):\n",
    "#         \"\"\"\n",
    "#         imgs: (N, 8, H, W)\n",
    "#         x: (N, L, patch_size**2 *3)\n",
    "#         \"\"\"\n",
    "#         p = self.patch_embed.patch_size[0]\n",
    "#         assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "#         h = w = imgs.shape[2] // p\n",
    "#         x = imgs.reshape(shape=(imgs.shape[0], 8, h, p, w, p))\n",
    "#         x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "#         x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 8))\n",
    "#         return x\n",
    "\n",
    "#     def unpatchify(self, x):\n",
    "#         \"\"\"\n",
    "#         x: (N, L, patch_size**2 *3)\n",
    "#         imgs: (N, 8, H, W)\n",
    "#         \"\"\"\n",
    "#         p = self.patch_embed.patch_size[0]\n",
    "#         h = w = int(x.shape[1]**.5)\n",
    "#         assert h * w == x.shape[1]\n",
    "        \n",
    "#         x = x.reshape(shape=(x.shape[0], h, w, p, p, 8))\n",
    "#         x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "#         imgs = x.reshape(shape=(x.shape[0], 8, h * p, h * p))\n",
    "#         return imgs\n",
    "\n",
    "#     def random_masking(self, x, mask_ratio):\n",
    "#         \"\"\"\n",
    "#         Perform per-sample random masking by per-sample shuffling.\n",
    "#         Per-sample shuffling is done by argsort random noise.\n",
    "#         x: [N, L, D], sequence\n",
    "#         \"\"\"\n",
    "#         N, L, D = x.shape  # batch, length, dim\n",
    "#         len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "#         noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "#         # sort noise for each sample\n",
    "#         ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "#         ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "#         # keep the first subset\n",
    "#         ids_keep = ids_shuffle[:, :len_keep]\n",
    "#         x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "#         # generate the binary mask: 0 is keep, 1 is remove\n",
    "#         mask = torch.ones([N, L], device=x.device)\n",
    "#         mask[:, :len_keep] = 0\n",
    "#         # unshuffle to get the binary mask\n",
    "#         mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "#         return x_masked, mask, ids_restore\n",
    "\n",
    "#     def forward_encoder(self, x, mask_ratio):\n",
    "#         # embed patches\n",
    "#         x = self.patch_embed(x)\n",
    "\n",
    "#         # add pos embed w/o cls token\n",
    "#         x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "#         # masking: length -> length * mask_ratio\n",
    "#         x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "#         # append cls token\n",
    "#         cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "#         cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "#         x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "#         # apply Transformer blocks\n",
    "#         for blk in self.blocks:\n",
    "#             x = blk(x)\n",
    "#         x = self.norm(x)\n",
    "\n",
    "#         return x, mask, ids_restore\n",
    "\n",
    "#     def forward_decoder(self, x, ids_restore):\n",
    "#         # embed tokens\n",
    "#         x = self.decoder_embed(x)\n",
    "\n",
    "#         # append mask tokens to sequence\n",
    "#         mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "#         x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "#         x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "#         x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "#         # add pos embed\n",
    "#         x = x + self.decoder_pos_embed\n",
    "\n",
    "#         # apply Transformer blocks\n",
    "#         for blk in self.decoder_blocks:\n",
    "#             x = blk(x)\n",
    "#         x = self.decoder_norm(x)\n",
    "\n",
    "#         # predictor projection\n",
    "#         x = self.decoder_pred(x)\n",
    "\n",
    "#         # remove cls token\n",
    "#         x = x[:, 1:, :]\n",
    "\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, imgs, mask_ratio=0.75):\n",
    "#         latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
    "#         pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
    "# #         loss = self.forward_loss(imgs, pred, mask)\n",
    "#         imgs = self.patchify(imgs)\n",
    "#         return imgs, pred, mask\n",
    "\n",
    "\n",
    "# # def mae_vit_base_patch16_dec512d8b(**kwargs):\n",
    "# #     model = MaskedAutoencoderViT(\n",
    "# #         img_size=125,patch_size=5, embed_dim=768, depth=12, num_heads=12,\n",
    "# #         decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "# #         mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "# #     return model\n",
    "\n",
    "\n",
    "# # # set recommended archs\n",
    "# # model = mae_vit_base_patch16_dec512d8b (img_size = 125) # decoder: 512 dim, 8 blocks\n",
    "# def mae_vit_base_patch16_dec512d8b(img_size=125, **kwargs):\n",
    "#     model = MaskedAutoencoderViT(\n",
    "#         img_size=img_size, patch_size=5, embed_dim=768, depth=8, num_heads=12,\n",
    "#         decoder_embed_dim=512, decoder_depth=4, decoder_num_heads=16,\n",
    "#         mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "#     return model\n",
    "\n",
    "# # Instantiate the model\n",
    "# model = mae_vit_base_patch16_dec512d8b(img_size=125)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4495984,
     "sourceId": 7701936,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4605304,
     "sourceId": 7852519,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3038.362949,
   "end_time": "2024-03-18T13:10:57.836829",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-18T12:20:19.473880",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
